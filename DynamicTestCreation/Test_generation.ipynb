{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Test Creation\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lib Imports: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY1\"] = \"\"  
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructing GPT to Develop a Multiple-Choice Quiz \n",
    "\n",
    "To have GPT create a multiple-choice quiz, we must provide clear instructions, including the subject matter, the number of possible answer choices, and the total questions. Additionally, GPT should include the correct answer to facilitate automatic grading later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_quiz_instructions(topic, question_count, answer_choices_count):\n",
    "        instructions = (f\"Generate a multiple-choice quiz on {topic} with {question_count} questions. \"\n",
    "        f\"Provide {answer_choices_count} options for each question. \"\n",
    "        f\"Include the correct answer for every question, starting with 'Correct Answer: '.\")\n",
    "        return instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_quiz_instructions(\"Python\", 4, 4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI API Invocation  \n",
    "For standard text generation, we will utilize the text-davinci-003 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.Completion.create(engine=\"text-davinci-003\",\n",
    "                                            prompt=generate_quiz_instructions(\"Python\", 4, 4),\n",
    "                                            max_tokens=256,\n",
    "                                            temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"choices\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Questions and Answers\n",
    "\n",
    "It's essential to extract the questions and answers from the generated quiz to present them to students in an organized manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_student_perspective(quiz, total_questions):\n",
    "    student_perspective = {1: \"\"}\n",
    "    question_index = 1\n",
    "    for line in quiz.split(\"\\n\"):\n",
    "        if not line.startswith(\"Correct Answer:\"):\n",
    "            student_perspective[question_index] += line + \"\\n\"\n",
    "        else:\n",
    "            if question_index < total_questions:\n",
    "                question_index += 1\n",
    "                student_perspective[question_index] = \"\"\n",
    "    return student_perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_student_perspective(response[\"choices\"][0][\"text\"], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_student_perspective(response[\"choices\"][0][\"text\"], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_correct_answers(quiz, total_questions):\n",
    "    correct_answers = {1: \"\"}\n",
    "    question_index = 1\n",
    "    for line in quiz.split(\"\\n\"):\n",
    "        if line.startswith(\"Correct Answer:\"):\n",
    "            correct_answers[question_index] += line + \"\\n\"\n",
    "\n",
    "            if question_index < total_questions:\n",
    "                        question_index += 1\n",
    "                        correct_answers[question_index] = \"\"\n",
    "    return correct_answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_correct_answers(response[\"choices\"][0][\"text\"], 4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Quiz Simulation    \n",
    "Utilizing the extracted questions, we can proceed with simulating the quiz experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def participate(student_perspective):\n",
    "    submitted_answers = {}\n",
    "    for question_num, question_content in student_perspective.items():\n",
    "        print(question_content)\n",
    "        response = input(\"Enter your answer: \")\n",
    "        submitted_answers[question_num] = response\n",
    "    return submitted_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_answers = participate(generate_student_perspective(response[\"choices\"][0][\"text\"], 4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Automated Assessment\n",
    "Utilizing the student's submitted answers and the known correct answers, we can now evaluate the quiz!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(known_answers, submitted_answers):\n",
    "    accurate_responses = 0\n",
    "    for question_num, response in submitted_answers.items():\n",
    "        if response.upper() == known_answers[question_num].upper()[16]:\n",
    "             accurate_responses += 1\n",
    "    score = 100 * accurate_responses / len(submitted_answers)\n",
    "    if score < 60:\n",
    "        pass_status = \"Not passed!\"\n",
    "    else:\n",
    "        pass_status = \"Passed!\"\n",
    "    return f\"{accurate_responses} out of {len(submitted_answers)} correct! You achieved: {score}% : {pass_status}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(retrieve_correct_answers(response[\"choices\"][0][\"text\"], 4), student_answers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algoquant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
